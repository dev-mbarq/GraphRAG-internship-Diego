{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Igual se lo meto en manual... no estoy seguro que me aporta este método si luego proveo el embedding para búsqueda yo mismo...\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_by_vector(embedding)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"How many distribution centers does Nike have in the US?\",\n",
    "        \"When was Nike incorporated?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB + Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama==0.4.7\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.12/site-packages (from ollama==0.4.7) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/site-packages (from ollama==0.4.7) (2.10.6)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama==0.4.7) (4.8.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama==0.4.7) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama==0.4.7) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama==0.4.7) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama==0.4.7) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama==0.4.7) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama==0.4.7) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama==0.4.7) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama==0.4.7) (1.3.1)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.4.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install chromadb==0.5.23\n",
    "#!pip install ollama==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import chromadb\n",
    "import torch\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from ollama import Client\n",
    "\n",
    "# Add \"src\" path to Python path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Import custom embedding function\n",
    "from embedding_utils import get_ollama_embedding\n",
    "from sage_utils import get_new_sage_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph\n",
    "with open(\"../data/MultiHop_graph_w_sage100_embeddings.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'category', 'source', 'author', 'text', 'embedding', 'SAGE_embedding'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[\"100_chunk_0\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection\n",
    "collection_semrag = chroma_client.create_collection(name=\"semantic_rag\")\n",
    "\n",
    "# Iterate over the nodes and filter those with type \"chunk\"\n",
    "for node_id in G.nodes:\n",
    "    node_data = G.nodes[node_id]\n",
    "    if node_data.get(\"type\") == \"chunk\":\n",
    "        doc_text = node_data.get(\"text\", \"\")\n",
    "        doc_embedding = node_data.get(\"embedding\", None)\n",
    "        \n",
    "        # Ensure that doc_embedding is a list (or array) of floats\n",
    "        # Add the node to the collection by specifying IDs, documents, and embeddings\n",
    "        collection_semrag.add(\n",
    "            ids=[str(node_id)],         # The ID will be the node's ID (converted to a string)\n",
    "            documents=[doc_text],         # The node's text\n",
    "            embeddings=[doc_embedding]    # The precomputed embedding you already have\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a query embedding 'query_embedding'\n",
    "query = \"Some query here\"\n",
    "query_bgem3 = get_ollama_embedding(query)[\"embedding\"]\n",
    "query_sage = get_new_sage_embedding(torch.tensor(query_bgem3, dtype=torch.float32).reshape(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection_semrag.query(\n",
    "    query_embeddings=[query_sage],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "# WIP\n",
    "processed_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom ebeddings with CHromaDB\n",
    "#from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "#\n",
    "#class MyEmbeddingFunction(EmbeddingFunction):\n",
    "#    def __call__(self, input: Documents) -> Embeddings:\n",
    "#        # embed the documents somehow\n",
    "#        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Querying collection with custom embeddings\n",
    "#collection.query(\n",
    "#    query_embeddings=[[11.1, 12.1, 13.1],[1.1, 2.3, 3.2], ...],\n",
    "#    n_results=10,\n",
    "#    where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "#    where_document={\"$contains\":\"search_string\"}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  #headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model='hf.co/unsloth/QwQ-32B-GGUF:Q6_K', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': f\"\"\"\n",
    "You are an expert assistant whose task is to answer the user's question using the provided context. Use the context to support your answer and ensure that your response is clear, accurate, and concise.\n",
    "\n",
    "Context:\n",
    "{processed_results}\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
