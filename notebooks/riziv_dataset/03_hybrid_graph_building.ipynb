{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the base document graph and extracting relevant legal concepts for a sample of text fragments, this notebook focuses on incorporating these concepts into the graph as a new type of node: concept nodes, each representing one of the detected legal terms. These nodes are connected via edges to all text chunk nodes in which the corresponding concept was identified.\n",
    "\n",
    "Additionally, before adding this new node type, the notebook includes a code section dedicated to pruning all sequence nodes of type PUB. Sequence nodes in the \"Sequence\" table of the DOCLEG database can be of two types: PUB and VIG, indicating whether the sequence captures the text at the moment of its publication (PUB) or at the time it entered into force (VIG) as well as the date when this happenned. During the hackathon, the RIZIV team expressed a preference for keeping only VIG sequences in order to simplify the prototype Q&A system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    # This will work in scripts where __file__ is defined\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    # Assuming \"src\" is parallel to the script folder\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "except NameError:\n",
    "    # In notebooks __file__ is not defined: assume we're in notebooks/riziv_dataset/\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the RIZIV dataset files\n",
    "RIZIV_data_path = os.path.join(project_root, \"data\", \"RIZIV_hackathon_main\")\n",
    "\n",
    "# Load the base document graph\n",
    "with open(os.path.join(RIZIV_data_path, 'base_document_graph.pkl'), 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Load the relevant legal concepts dataframe\n",
    "# Note: the file being loaded is not the df_concepts.pkl file, but the df_concepts_hackathon_original.pkl file, \n",
    "# as the latter contains the original concepts dataframe used during the hackathon, which is not the same as \n",
    "# the one generated in notebook 02.\n",
    "concepts_df = pd.read_pickle(os.path.join(RIZIV_data_path, 'df_concepts_hackathon_original.pkl'))\n",
    "\n",
    "# Resolve .env path relative to this script, regardless of where it's run from\n",
    "env_path = os.path.join(RIZIV_data_path, '.env')\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Removal of sequence nodes of type \"PUB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes care of the deletion of 'PUB' type sequence nodes from the base document graph as well as all their dependen text chunk nodes. A summary of the graph number of nodes by type is printed before and after this operation to illustrate its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Graph State\n",
      "--------------------------------------------------\n",
      "Nodes of type 'act': 1146\n",
      "Nodes of type 'article': 9244\n",
      "Nodes of type 'sequence': 31941\n",
      "Nodes of type 'text_chunk': 43436\n",
      "Total edges: 192232\n",
      "--------------------------------------------------\n",
      "\n",
      "Graph State After Removing PUB Sequences and Their Chunks\n",
      "--------------------------------------------------\n",
      "Nodes of type 'act': 1146\n",
      "Nodes of type 'article': 9244\n",
      "Nodes of type 'sequence': 15857\n",
      "Nodes of type 'text_chunk': 22127\n",
      "Total edges: 106996\n",
      "--------------------------------------------------\n",
      "\n",
      "Removed 16084 PUB sequence nodes\n",
      "Removed 21309 associated chunk nodes\n",
      "Total nodes removed: 37393\n"
     ]
    }
   ],
   "source": [
    "# Removing 'PUB' sequence nodes from the graph as well as their dependent text_chunk nodes\n",
    "\n",
    "def print_graph_summary(G, message=\"Graph Summary\"):\n",
    "    \"\"\"Print summary of nodes by type\"\"\"\n",
    "    print(f\"\\n{message}\")\n",
    "    print(\"-\" * 50)\n",
    "    node_types = {}\n",
    "    for node in G.nodes():\n",
    "        node_type = G.nodes[node].get('type_node')\n",
    "        if node_type:\n",
    "            node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    for node_type, count in sorted(node_types.items()):\n",
    "        print(f\"Nodes of type '{node_type}': {count}\")\n",
    "    print(f\"Total edges: {G.number_of_edges()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print initial graph summary\n",
    "print_graph_summary(G, \"Initial Graph State\")\n",
    "\n",
    "# Get list of sequence nodes to remove\n",
    "sequence_nodes_to_remove = [\n",
    "    node for node in G.nodes() \n",
    "    if G.nodes[node].get('type_node') == 'sequence' \n",
    "    and G.nodes[node].get('type') == 'PUB'\n",
    "]\n",
    "\n",
    "# Get all chunk nodes connected to these sequences\n",
    "chunk_nodes_to_remove = set()\n",
    "for seq_node in sequence_nodes_to_remove:\n",
    "    # Get all neighbors of the sequence node\n",
    "    neighbors = G.neighbors(seq_node)\n",
    "    # Add to removal list if it's a text_chunk\n",
    "    for neighbor in neighbors:\n",
    "        if G.nodes[neighbor].get('type_node') == 'text_chunk':\n",
    "            chunk_nodes_to_remove.add(neighbor)\n",
    "\n",
    "# Remove all identified nodes\n",
    "all_nodes_to_remove = sequence_nodes_to_remove + list(chunk_nodes_to_remove)\n",
    "G.remove_nodes_from(all_nodes_to_remove)\n",
    "\n",
    "# Print final graph summary\n",
    "print_graph_summary(G, \"Graph State After Removing PUB Sequences and Their Chunks\")\n",
    "\n",
    "print(f\"\\nRemoved {len(sequence_nodes_to_remove)} PUB sequence nodes\")\n",
    "print(f\"Removed {len(chunk_nodes_to_remove)} associated chunk nodes\")\n",
    "print(f\"Total nodes removed: {len(all_nodes_to_remove)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing the extracted relevant legal concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing the new nodes and edges into the graph, the DataFrame containing the relevant legal concepts undergoes a few pre-processing steps. The first two are sanity checks motivated by the fact that the base document graph was pruned (removal of PUB sequence nodes) after the automated extraction of relevant concepts. As a result, any references to nodes that no longer exist in the graph need to be removed.\n",
    "\n",
    "Additionally, to limit the number of new nodes added and prioritize only those that could serve as bridges between different sub-graphs or \"neighborhoods\" within the base graph, only concepts cited in at least two distinct text chunk nodes are retained â€” i.e., concept nodes must have at least two edges pointing to different text chunks to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering concepts after graph pruning...\n",
      "Original concepts DataFrame shape: (4976, 4)\n",
      "Original average chunks per concept: 1.41\n",
      "\n",
      "Filtering results:\n",
      "New DataFrame shape: (285, 4)\n",
      "New average chunks per concept: 3.95\n",
      "\n",
      "Concepts retention statistics:\n",
      "- After removing non-existing chunks: 285 concepts\n",
      "- After requiring minimum 2 chunks: 285 concepts\n",
      "Final retention rate: 5.73% of original concepts\n",
      "\n",
      "Distribution of chunk mentions in final concepts:\n",
      "count    285.000000\n",
      "mean       3.947368\n",
      "std        4.732347\n",
      "min        2.000000\n",
      "25%        2.000000\n",
      "50%        2.000000\n",
      "75%        4.000000\n",
      "max       42.000000\n",
      "Name: chunk_list, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create filtered dataframe after pruning\n",
    "print(\"Filtering concepts after graph pruning...\")\n",
    "print(f\"Original concepts DataFrame shape: {concepts_df.shape}\")\n",
    "print(f\"Original average chunks per concept: {concepts_df['chunk_list'].str.len().mean():.2f}\")\n",
    "\n",
    "# Sanity check 1: Create new DataFrame with filtered chunk lists\n",
    "concepts_after_pruning_df = concepts_df.copy()\n",
    "concepts_after_pruning_df['chunk_list'] = concepts_after_pruning_df['chunk_list'].apply(\n",
    "    lambda chunks: [chunk for chunk in chunks if G.has_node(chunk)]\n",
    ")\n",
    "\n",
    "# Sanity check 2: Remove concepts with empty chunk lists \n",
    "concepts_after_pruning_df = concepts_after_pruning_df[\n",
    "    concepts_after_pruning_df['chunk_list'].str.len() > 0\n",
    "]\n",
    "\n",
    "# Filter: Keep only concepts mentioned in at least 2 chunks (reduces the number of concepts to be added to the graph)\n",
    "concepts_after_pruning_df = concepts_after_pruning_df[\n",
    "    concepts_after_pruning_df['chunk_list'].str.len() >= 2\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nFiltering results:\")\n",
    "print(f\"New DataFrame shape: {concepts_after_pruning_df.shape}\")\n",
    "print(f\"New average chunks per concept: {concepts_after_pruning_df['chunk_list'].str.len().mean():.2f}\")\n",
    "print(\"\\nConcepts retention statistics:\")\n",
    "print(f\"- After removing non-existing chunks: {len(concepts_after_pruning_df[concepts_after_pruning_df['chunk_list'].str.len() > 0])} concepts\")\n",
    "print(f\"- After requiring minimum 2 chunks: {len(concepts_after_pruning_df)} concepts\")\n",
    "print(f\"Final retention rate: {(len(concepts_after_pruning_df)/len(concepts_df))*100:.2f}% of original concepts\")\n",
    "\n",
    "# Distribution of mentions\n",
    "print(\"\\nDistribution of chunk mentions in final concepts:\")\n",
    "mentions_dist = concepts_after_pruning_df['chunk_list'].str.len().describe()\n",
    "print(mentions_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding concept nodes to the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the filtered DataFrame of relevant legal concepts is used to add new concept nodes to the document graph. Each row in the DataFrame corresponds to one concept node, which is added to the graph with its associated attributes, including the concept's name and its list of categories.\n",
    "\n",
    "For each concept, bidirectional edges are then created between the concept node and all the text chunk nodes where the concept was detected. These edges are labeled according to their direction (chunk_cites_concept and concept_cited_in_chunk) and allow the newly added nodes to connect different areas of the base graph, potentially bridging separate sub-graphs.\n",
    "\n",
    "A summary of the operation is printed upon completion, including the total number of concepts added, the total number of edges created, and the average number of edges per concept. Additionally, the distribution of edge types in the graph is reported, along with an updated summary of the graphâ€™s structure after the integration of the concept nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding concepts to graph...\n",
      "\n",
      "Before adding concepts\n",
      "--------------------------------------------------\n",
      "Nodes of type 'act': 1146\n",
      "Nodes of type 'article': 9244\n",
      "Nodes of type 'sequence': 15857\n",
      "Nodes of type 'text_chunk': 22127\n",
      "Total edges: 106996\n",
      "--------------------------------------------------\n",
      "Initial state: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing concepts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 2774.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concepts integration summary:\n",
      "Concepts added: 285\n",
      "Total edges added: 2250 (1125 pairs of bidirectional edges)\n",
      "Average edges per concept: 7.89 (3.95 pairs)\n",
      "\n",
      "Edge types distribution:\n",
      "- contains_article: 9244\n",
      "- belongs_to_act: 9244\n",
      "- has_version: 15857\n",
      "- version_of: 15857\n",
      "- contains_chunk: 22127\n",
      "- contained_in_sequence: 22127\n",
      "- chunk_cites_concept: 1125\n",
      "- followed_by: 6270\n",
      "- preceded_by: 6270\n",
      "- concept_cited_in_chunk: 1125\n",
      "\n",
      "After adding concepts\n",
      "--------------------------------------------------\n",
      "Nodes of type 'act': 1146\n",
      "Nodes of type 'article': 9244\n",
      "Nodes of type 'concept': 285\n",
      "Nodes of type 'sequence': 15857\n",
      "Nodes of type 'text_chunk': 22127\n",
      "Total edges: 109246\n",
      "--------------------------------------------------\n",
      "\n",
      "Final state: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_concepts_to_graph(G, concepts_after_pruning_df):\n",
    "    \"\"\"Add concept nodes to graph and connect them to their chunks with bidirectional edges\"\"\"\n",
    "    \n",
    "    print(\"\\nAdding concepts to graph...\")\n",
    "    print(\"Initial state:\", print_graph_summary(G, \"Before adding concepts\"))\n",
    "    \n",
    "    concepts_added = 0\n",
    "    total_edges_added = 0\n",
    "    \n",
    "    # Iterate over filtered concepts DataFrame\n",
    "    for idx, row in tqdm(concepts_after_pruning_df.iterrows(), \n",
    "                        total=len(concepts_after_pruning_df), \n",
    "                        desc=\"Processing concepts\"):\n",
    "        # Create concept node ID\n",
    "        concept_id = f\"concept_{idx}\"\n",
    "        \n",
    "        # Add concept node with all its attributes\n",
    "        G.add_node(\n",
    "            concept_id,\n",
    "            type_node='concept',\n",
    "            name=row['concept_name'],\n",
    "            categories=row['category_list'],\n",
    "        )\n",
    "        \n",
    "        # Add bidirectional edges to all chunks\n",
    "        for chunk in row['chunk_list']:\n",
    "            # Edge from chunk to concept\n",
    "            G.add_edge(chunk, concept_id, relationship_type='chunk_cites_concept')\n",
    "            # Edge from concept to chunk\n",
    "            G.add_edge(concept_id, chunk, relationship_type='concept_cited_in_chunk')\n",
    "        \n",
    "        concepts_added += 1\n",
    "        total_edges_added += 2 * len(row['chunk_list'])  # Multiply by 2 for bidirectional edges\n",
    "    \n",
    "    print(\"\\nConcepts integration summary:\")\n",
    "    print(f\"Concepts added: {concepts_added}\")\n",
    "    print(f\"Total edges added: {total_edges_added} ({total_edges_added//2} pairs of bidirectional edges)\")\n",
    "    print(f\"Average edges per concept: {total_edges_added/concepts_added:.2f} ({(total_edges_added/2)/concepts_added:.2f} pairs)\")\n",
    "    \n",
    "    # Count edges by type\n",
    "    edge_types = {}\n",
    "    for _, _, edge_data in G.edges(data=True):\n",
    "        edge_type = edge_data.get('relationship_type', 'unknown')\n",
    "        edge_types[edge_type] = edge_types.get(edge_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nEdge types distribution:\")\n",
    "    for edge_type, count in edge_types.items():\n",
    "        print(f\"- {edge_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nFinal state:\", print_graph_summary(G, \"After adding concepts\"))\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Execute the function\n",
    "G = add_concepts_to_graph(G, concepts_after_pruning_df)\n",
    "\n",
    "# Save the graph\n",
    "with open(os.path.join(RIZIV_data_path, 'embeddingless_base_hybrid_graph.pkl'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311_graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
