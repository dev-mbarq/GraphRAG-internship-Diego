{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained a first version of the hybrid graph, the final step before producing node embeddings under the proposed approach is to assign a base semantic embedding to each node in the graph. This notebook covers that process, which is carried out in ascending order according to the hierarchical level of each node type within the DOCLEG document structure. That is, embeddings are first assigned to concept and text chunk nodes, followed by sequence, article, and finally act nodes.\n",
    "\n",
    "This order is not arbitrary — it reflects the need to begin with nodes that are directly associated with actual text content, from which semantic embeddings can be meaningfully derived. Within the base graph constructed during the hackathon, only text chunk nodes (and to some extent concept and act nodes, as will be discussed later) meet this condition.\n",
    "\n",
    "Due to the time constraints of the hackathon, the semantic embeddings of text chunk nodes are used as the foundation to propagate embeddings to their connected sequence nodes, and from there to article nodes, and partially to act nodes. This approach provided a quick and practical solution to assign initial embeddings to all nodes in the graph, enabling the use of a GraphSAGE model for node embedding generation.\n",
    "\n",
    "Nonetheless, it is worth noting that, given more time, it would have been preferable to explore alternative strategies capable of producing unique embeddings for sequence and/or article nodes based on their own content — for example, by generating textual summaries at these levels — rather than relying solely on the aggregation of neighboring node embeddings.\n",
    "\n",
    "Another area of potential improvement/refinement is the model employed for embedding generation in the first place. Due to its convenience, OpenAI's embedding models avilable on azure were employed in this case. However the use of BERT-based embedding models especialized on legal texts, text in DUtch and/or French, or both could further improve thequality of embeddings potneially impacting in a positive manner the retrieval of relevant evidence for answering a user's query. \n",
    "\n",
    "The final product of this notebook is a graph where all its nodes have a semantic embedding assigned, enabling its use to porduce node embddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "# azure.ai.inference import EmbeddingsClient\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "try:\n",
    "    # This will work in scripts where __file__ is defined\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    # Assuming \"src\" is parallel to the script folder\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "except NameError:\n",
    "    # In notebooks __file__ is not defined: assume we're in notebooks/riziv_dataset/\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from main.ollama_utils import get_ollama_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the RIZIV dataset files\n",
    "RIZIV_data_path = os.path.join(project_root, \"data\", \"RIZIV_hackathon_main\")\n",
    "\n",
    "# Load the graph\n",
    "with open(os.path.join(RIZIV_data_path, 'intermediate','embeddingless_base_hybrid_graph.pkl'), 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Resolve .env path relative to this script, regardless of where it's run from\n",
    "env_path = os.path.join(RIZIV_data_path, '.env')\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate semantic embeddings for Text Chunk and Concept nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers the generation and assignment of semantic embeddings to all text chunk and concept nodes in the graph. For the hackathon, embeddings were generated using OpenAI's text-embedding-3-small model via the Azure API.\n",
    "\n",
    "For text chunk nodes, embeddings are computed based on the full text associated with each node. In the case of concept nodes, embeddings are derived from the concept's name. This choice, while functional, should be considered sub-optimal. It was mainly driven by the time constraints of the hackathon and the fact that concept nodes were included primarily for illustrative purposes (see Notebook 02). A significantly more robust approach would have involved generating a concise, representative definition for each relevant concept, effectively building a dictionary of key legal terms extracted from the DOCLEG database. These definitions could then serve as a more meaningful basis for computing the semantic embeddings of concept nodes.\n",
    "\n",
    "Once the embedding generation process is complete, a summary is printed including the number of nodes processed, the number successfully assigned an embedding, and the overall coverage rate across the targeted node types. Some nodes may fail to receive embeddings due to occasional issues such as API errors, content filtering constraints, or unexpected artifacts within the source text, all plausible given the size and heterogeneity of the DOCLEG dataset. In such cases, an empty embedding is assigned to the corresponding node. This is again a quick fix within the contex of the hackathon. With more time available an ideal fix would have been to closely inspect which may be the actual source of problems in each case and depending on the case apply a tailored solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05666721, -0.99160188, -0.19772267, ...,  0.03814373,\n",
       "       -1.27074277,  1.15008116])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Testing embeddings generation with examples\n",
    "# \n",
    "# # For Serverless API or Managed Compute endpoints\n",
    "# client = AzureOpenAI(   \n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\"),\n",
    "#     azure_ad_token=AzureKeyCredential(os.getenv(\"AZURE_OPENAI_EMBEDDINGS_KEY\")),\n",
    "# )\n",
    "# \n",
    "# response = client.embeddings.create(\n",
    "#     input=[\"first phrase\",\"second phrase\",\"third phrase\"],\n",
    "#     model=os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "# )\n",
    "# \n",
    "# for item in response.data:\n",
    "#     item_embedding = item['embedding']\n",
    "#     print(item_embedding)\n",
    "#     print(len(item_embedding))\n",
    "\n",
    "embedding = np.array(get_ollama_embedding(\"Embedding generation test\", )['embedding'])\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for nodes...\n",
      "Found 22127 text chunks and 285 concepts\n",
      "\n",
      "Processing text chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 878/1383 [17:03<07:50,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch starting with node chunk_12018556_0: (None) '$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\n",
      "Code: None\n",
      "Message: '$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [41:25<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing concept nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation summary:\n",
      "Total target nodes: 22412\n",
      "Nodes with real embeddings: 22396\n",
      "Nodes with empty embeddings: 16\n",
      "Nodes without embeddings: 0\n",
      "Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings_for_nodes(G, client, assign_empty: bool = True):\n",
    "    \"\"\"\n",
    "    Generate and assign embeddings for text_chunk and concept nodes.\n",
    "    If a node cannot receive an embedding and assign_empty is True,\n",
    "    it will receive an empty embedding of the appropriate dimension.\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings for nodes...\")\n",
    "    \n",
    "    # Get lists of nodes by type\n",
    "    chunk_nodes = [node for node in G.nodes() if G.nodes[node].get('type_node') == 'text_chunk']\n",
    "    concept_nodes = [node for node in G.nodes() if G.nodes[node].get('type_node') == 'concept']\n",
    "    target_nodes = set(chunk_nodes + concept_nodes)  # Use set to ensure uniqueness\n",
    "    \n",
    "    print(f\"Found {len(chunk_nodes)} text chunks and {len(concept_nodes)} concepts\")\n",
    "    \n",
    "    # Clear existing embeddings for target nodes\n",
    "    for node in target_nodes:\n",
    "        if 'embedding' in G.nodes[node]:\n",
    "            del G.nodes[node]['embedding']\n",
    "    \n",
    "    embedding_dim = None\n",
    "    nodes_with_empty = 0\n",
    "    processed_nodes = set()  # Track which nodes we've processed\n",
    "    \n",
    "    # Process text chunks\n",
    "    print(\"\\nProcessing text chunks...\")\n",
    "    for batch in tqdm(range(0, len(chunk_nodes), 16)):\n",
    "        batch_nodes = chunk_nodes[batch:batch + 16]\n",
    "        batch_texts = [G.nodes[node]['text'] for node in batch_nodes]\n",
    "        \n",
    "        try:\n",
    "            response = client.embed(\n",
    "                input=batch_texts,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            \n",
    "            for node, item in zip(batch_nodes, response.data):\n",
    "                embedding = np.array(item['embedding'])\n",
    "                G.nodes[node]['embedding'] = embedding\n",
    "                processed_nodes.add(node)\n",
    "                if embedding_dim is None:\n",
    "                    embedding_dim = len(embedding)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with node {batch_nodes[0]}: {str(e)}\")\n",
    "            if assign_empty and embedding_dim is not None:\n",
    "                for node in batch_nodes:\n",
    "                    if node not in processed_nodes:\n",
    "                        G.nodes[node]['embedding'] = np.zeros(embedding_dim)\n",
    "                        nodes_with_empty += 1\n",
    "                        processed_nodes.add(node)\n",
    "            continue\n",
    "    \n",
    "    # Process concept nodes\n",
    "    print(\"\\nProcessing concept nodes...\")\n",
    "    for batch in tqdm(range(0, len(concept_nodes), 16)):\n",
    "        batch_nodes = concept_nodes[batch:batch + 16]\n",
    "        batch_texts = [G.nodes[node]['name'] for node in batch_nodes]\n",
    "        \n",
    "        try:\n",
    "            response = client.embed(\n",
    "                input=batch_texts,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            \n",
    "            for node, item in zip(batch_nodes, response.data):\n",
    "                embedding = np.array(item['embedding'])\n",
    "                G.nodes[node]['embedding'] = embedding\n",
    "                processed_nodes.add(node)\n",
    "                if embedding_dim is None:\n",
    "                    embedding_dim = len(embedding)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with node {batch_nodes[0]}: {str(e)}\")\n",
    "            if assign_empty and embedding_dim is not None:\n",
    "                for node in batch_nodes:\n",
    "                    if node not in processed_nodes:\n",
    "                        G.nodes[node]['embedding'] = np.zeros(embedding_dim)\n",
    "                        nodes_with_empty += 1\n",
    "                        processed_nodes.add(node)\n",
    "            continue\n",
    "    \n",
    "    # Verify embeddings (solo para nodos objetivo)\n",
    "    nodes_with_real_embeddings = sum(1 for node in target_nodes \n",
    "                                   if 'embedding' in G.nodes[node] \n",
    "                                   and not np.all(G.nodes[node]['embedding'] == 0))\n",
    "    \n",
    "    nodes_without_embeddings = len(target_nodes) - len(processed_nodes)\n",
    "    \n",
    "    print(\"\\nEmbedding generation summary:\")\n",
    "    print(f\"Total target nodes: {len(target_nodes)}\")\n",
    "    print(f\"Nodes with real embeddings: {nodes_with_real_embeddings}\")\n",
    "    print(f\"Nodes with empty embeddings: {nodes_with_empty}\")\n",
    "    print(f\"Nodes without embeddings: {nodes_without_embeddings}\")\n",
    "    print(f\"Coverage: {((nodes_with_real_embeddings + nodes_with_empty)/len(target_nodes))*100:.2f}%\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Execute the function\n",
    "G = generate_embeddings_for_nodes(G, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Propagate embeddings to Sequence and Article nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having generated semantic embeddings for Text Chunk nodes, these are then propagated to their parent sequence nodes, and subsequently from sequences to their corresponding article nodes. The embeddings or \"parent\" nodes are computed by averaging the embeddings of all connected nodes at the lower hierarchical level. That is, each sequence node receives an embedding based on the average of its associated text chunks, and each article node does so based on the average of its linked sequences.\n",
    "\n",
    "At the end of this process, all sequence and article nodes have been assigned an embedding, either real or empty, preparing the graph for the final embedding step at the act level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating embeddings to sequences...\n",
      "\n",
      "Total sequence nodes: 15857\n",
      "Sequences with propagated embedding: 15857\n",
      "Sequences with empty embedding: 0\n",
      "Sequences without embedding: 0\n",
      "Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def propagate_embeddings_to_sequences(G: nx.Graph, assign_empty: bool = True):\n",
    "    \"\"\"\n",
    "    Propagate embeddings from text_chunk nodes to their parent sequence nodes\n",
    "    by averaging the embeddings of all connected chunks. Optionally assign\n",
    "    empty embeddings to sequences without valid chunk embeddings.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): Graph to modify\n",
    "        assign_empty (bool): Whether to assign empty embeddings to sequences without valid chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[int, int, int, List[str]]: (total_sequences, sequences_with_embedding, \n",
    "                                         sequences_with_empty, problem_sequences)\n",
    "    \"\"\"\n",
    "    # Get all sequence nodes\n",
    "    sequence_nodes = [\n",
    "        node for node, attr in G.nodes(data=True) \n",
    "        if attr.get('type_node') == 'sequence'\n",
    "    ]\n",
    "    \n",
    "    sequences_without_embedding = []\n",
    "    sequences_with_empty = []\n",
    "    total_sequences = len(sequence_nodes)\n",
    "    embedding_dim = None\n",
    "    \n",
    "    print(\"Propagating embeddings to sequences...\")\n",
    "    for sequence_node in sequence_nodes:\n",
    "        try:\n",
    "            # Get all neighboring chunks\n",
    "            chunk_neighbors = [\n",
    "                neighbor for neighbor in G.neighbors(sequence_node)\n",
    "                if G.nodes[neighbor].get('type_node') == 'text_chunk'\n",
    "            ]\n",
    "            \n",
    "            if not chunk_neighbors:\n",
    "                if assign_empty:\n",
    "                    # If no embedding dimension is known yet, try to find it from any chunk node\n",
    "                    if embedding_dim is None:\n",
    "                        for node in G.nodes():\n",
    "                            if (G.nodes[node].get('type_node') == 'text_chunk' and \n",
    "                                G.nodes[node].get('embedding') is not None):\n",
    "                                embedding_dim = len(G.nodes[node]['embedding'])\n",
    "                                break\n",
    "                    \n",
    "                    if embedding_dim is not None:\n",
    "                        G.nodes[sequence_node]['embedding'] = [0.0] * embedding_dim\n",
    "                        sequences_with_empty.append(sequence_node)\n",
    "                        continue\n",
    "                \n",
    "                sequences_without_embedding.append(sequence_node)\n",
    "                continue\n",
    "            \n",
    "            # Get embeddings from all chunks\n",
    "            chunk_embeddings = []\n",
    "            for chunk in chunk_neighbors:\n",
    "                embedding = G.nodes[chunk].get('embedding')\n",
    "                if embedding is not None:\n",
    "                    chunk_embeddings.append(embedding)\n",
    "                    if embedding_dim is None:\n",
    "                        embedding_dim = len(embedding)\n",
    "            \n",
    "            if not chunk_embeddings:\n",
    "                if assign_empty and embedding_dim is not None:\n",
    "                    G.nodes[sequence_node]['embedding'] = [0.0] * embedding_dim\n",
    "                    sequences_with_empty.append(sequence_node)\n",
    "                    continue\n",
    "                    \n",
    "                sequences_without_embedding.append(sequence_node)\n",
    "                continue\n",
    "            \n",
    "            # Calculate average embedding\n",
    "            avg_embedding = [\n",
    "                sum(values) / len(chunk_embeddings)\n",
    "                for values in zip(*chunk_embeddings)\n",
    "            ]\n",
    "            \n",
    "            # Assign average embedding to sequence node\n",
    "            G.nodes[sequence_node]['embedding'] = avg_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            if assign_empty and embedding_dim is not None:\n",
    "                G.nodes[sequence_node]['embedding'] = [0.0] * embedding_dim\n",
    "                sequences_with_empty.append(sequence_node)\n",
    "            else:\n",
    "                sequences_without_embedding.append(sequence_node)\n",
    "    \n",
    "    # Print summary\n",
    "    sequences_with_embedding = total_sequences - len(sequences_without_embedding) - len(sequences_with_empty)\n",
    "    \n",
    "    print(f\"\\nTotal sequence nodes: {total_sequences}\")\n",
    "    print(f\"Sequences with propagated embedding: {sequences_with_embedding}\")\n",
    "    print(f\"Sequences with empty embedding: {len(sequences_with_empty)}\")\n",
    "    print(f\"Sequences without embedding: {len(sequences_without_embedding)}\")\n",
    "    print(f\"Coverage: {((sequences_with_embedding + len(sequences_with_empty))/total_sequences)*100:.2f}%\")\n",
    "    \n",
    "    return total_sequences, sequences_with_embedding, len(sequences_with_empty), sequences_without_embedding\n",
    "\n",
    "total_seqeunces, sequences_with_embedding, n_sequences_with_empy, sequences_without_embedding = propagate_embeddings_to_sequences(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating embeddings to articles...\n",
      "\n",
      "Total article nodes: 9244\n",
      "Articles with propagated embedding: 9175\n",
      "Articles with empty embedding: 69\n",
      "Articles without any embedding: 0\n",
      "Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def propagate_embeddings_to_articles(G: nx.Graph, assign_empty: bool = True):\n",
    "    \"\"\"\n",
    "    Propagate embeddings from sequence nodes to their parent article nodes\n",
    "    by averaging the embeddings of all connected sequences. Optionally assign\n",
    "    empty embeddings to articles without sequence neighbors.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): Graph to modify\n",
    "        assign_empty (bool): Whether to assign empty embeddings to articles without sequences\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[int, int, int, List[str]]: (total_articles, articles_with_embedding, \n",
    "                                         articles_with_empty, problem_articles)\n",
    "    \"\"\"\n",
    "    # Get all article nodes\n",
    "    article_nodes = [\n",
    "        node for node, attr in G.nodes(data=True) \n",
    "        if attr.get('type_node') == 'article'\n",
    "    ]\n",
    "    \n",
    "    articles_without_embedding = []\n",
    "    articles_with_empty = []\n",
    "    total_articles = len(article_nodes)\n",
    "    embedding_dim = None\n",
    "    \n",
    "    print(\"Propagating embeddings to articles...\")\n",
    "    for article_node in article_nodes:\n",
    "        try:\n",
    "            # Get all neighboring sequences\n",
    "            sequence_neighbors = [\n",
    "                neighbor for neighbor in G.neighbors(article_node)\n",
    "                if G.nodes[neighbor].get('type_node') == 'sequence'\n",
    "            ]\n",
    "            \n",
    "            if not sequence_neighbors:\n",
    "                if assign_empty:\n",
    "                    # If no embedding dimension is known yet, try to find it from any sequence node\n",
    "                    if embedding_dim is None:\n",
    "                        for node in G.nodes():\n",
    "                            if (G.nodes[node].get('type_node') == 'sequence' and \n",
    "                                G.nodes[node].get('embedding') is not None):\n",
    "                                embedding_dim = len(G.nodes[node]['embedding'])\n",
    "                                break\n",
    "                    \n",
    "                    if embedding_dim is not None:\n",
    "                        G.nodes[article_node]['embedding'] = [0.0] * embedding_dim\n",
    "                        articles_with_empty.append(article_node)\n",
    "                        continue\n",
    "                \n",
    "                articles_without_embedding.append(article_node)\n",
    "                continue\n",
    "            \n",
    "            # Get embeddings from all sequences\n",
    "            sequence_embeddings = []\n",
    "            for sequence in sequence_neighbors:\n",
    "                embedding = G.nodes[sequence].get('embedding')\n",
    "                if embedding is not None:\n",
    "                    sequence_embeddings.append(embedding)\n",
    "                    if embedding_dim is None:\n",
    "                        embedding_dim = len(embedding)\n",
    "            \n",
    "            if not sequence_embeddings:\n",
    "                if assign_empty and embedding_dim is not None:\n",
    "                    G.nodes[article_node]['embedding'] = [0.0] * embedding_dim\n",
    "                    articles_with_empty.append(article_node)\n",
    "                    continue\n",
    "                \n",
    "                articles_without_embedding.append(article_node)\n",
    "                continue\n",
    "            \n",
    "            # Calculate average embedding\n",
    "            avg_embedding = [\n",
    "                sum(values) / len(sequence_embeddings)\n",
    "                for values in zip(*sequence_embeddings)\n",
    "            ]\n",
    "            \n",
    "            # Assign average embedding to article node\n",
    "            G.nodes[article_node]['embedding'] = avg_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            articles_without_embedding.append(article_node)\n",
    "    \n",
    "    # Print summary\n",
    "    articles_with_real_embedding = total_articles - len(articles_without_embedding) - len(articles_with_empty)\n",
    "    \n",
    "    print(f\"\\nTotal article nodes: {total_articles}\")\n",
    "    print(f\"Articles with propagated embedding: {articles_with_real_embedding}\")\n",
    "    print(f\"Articles with empty embedding: {len(articles_with_empty)}\")\n",
    "    print(f\"Articles without any embedding: {len(articles_without_embedding)}\")\n",
    "    print(f\"Coverage: {((articles_with_real_embedding + len(articles_with_empty))/total_articles)*100:.2f}%\")\n",
    "    \n",
    "    return total_articles, articles_with_real_embedding, len(articles_with_empty), articles_without_embedding\n",
    "\n",
    "total_articles, sequences_with_real_embedding, n_articles_with_empty, articles_without_embedding = propagate_embeddings_to_articles(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings for Act nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in assigning semantic embeddings across the graph involves the act nodes. Unlike other node types, acts do not contain rich textual content but do include some textual information in the form of their Title or Short format title with provide some elemental semantic value. The initial embeddings for Act nodes are generated based on these field (Title and TitleShor from the WorkActLanguageFR included as node attributes) when they are available, averaging them. Otherwise, the embedding is computed from whichever field is available. If neither yields a valid result, an empty embedding is assigned, ensuring coverage for all act nodes.\n",
    "\n",
    "To refine these initial embeddings, a second step is performed where each act node’s embedding is recalculated by averaging its embedding from the first step with the mean embedding of its neighboring article nodes. This, although sub-optimal again, aimed at allowing act node embeddings to better reflect their actual content and context within the broader document hierarchy, leveraging the semantic information already captured at lower levels of the graph.\n",
    "\n",
    "Having computed the resulting act embeddings, completing the embedding generation and propagation process, all nodes in the graph, from the most granular to the most abstract, are represented in the shared semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for act nodes...\n",
      "Found 1146 act nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing acts: 100%|██████████| 1146/1146 [05:09<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation summary:\n",
      "Total act nodes: 1146\n",
      "Acts with averaged embeddings: 1134 (98.95%)\n",
      "Acts with single embedding: 10 (0.87%)\n",
      "Acts with empty embeddings: 2 (0.17%)\n",
      "Embedding dimension: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings_for_acts(G: nx.Graph, client) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Generate and assign embeddings for act nodes based on their Title and TitleShort attributes.\n",
    "    Process one by one and handle failures gracefully.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): Graph containing act nodes\n",
    "        client: Azure OpenAI embeddings client\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph: Graph with updated act embeddings\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings for act nodes...\")\n",
    "    \n",
    "    # Get all act nodes\n",
    "    act_nodes = [node for node in G.nodes() if G.nodes[node].get('type_node') == 'act']\n",
    "    print(f\"Found {len(act_nodes)} act nodes\")\n",
    "    \n",
    "    # Track statistics\n",
    "    acts_with_averaged_embedding = 0\n",
    "    acts_with_single_embedding = 0\n",
    "    acts_with_empty_embedding = 0\n",
    "    embedding_dim = None\n",
    "    \n",
    "    # Process acts one by one\n",
    "    for node in tqdm(act_nodes, desc=\"Processing acts\"):\n",
    "        try:\n",
    "            title = G.nodes[node].get('Title', '')\n",
    "            short_title = G.nodes[node].get('TitleShort', '')\n",
    "            title_embedding = None\n",
    "            short_title_embedding = None\n",
    "            \n",
    "            # Try to get Title embedding\n",
    "            try:\n",
    "                title_response = client.embed(\n",
    "                    input=[title],\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "                title_embedding = np.array(title_response.data[0]['embedding'])\n",
    "                if embedding_dim is None:\n",
    "                    embedding_dim = len(title_embedding)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try to get TitleShort embedding\n",
    "            try:\n",
    "                short_title_response = client.embed(\n",
    "                    input=[short_title],\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "                short_title_embedding = np.array(short_title_response.data[0]['embedding'])\n",
    "                if embedding_dim is None:\n",
    "                    embedding_dim = len(short_title_embedding)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Assign embeddings based on what we got\n",
    "            if title_embedding is not None and short_title_embedding is not None:\n",
    "                # Store both embeddings and their average\n",
    "                G.nodes[node]['title_embedding'] = title_embedding\n",
    "                G.nodes[node]['title_short_embedding'] = short_title_embedding\n",
    "                G.nodes[node]['embedding'] = (title_embedding + short_title_embedding) / 2\n",
    "                acts_with_averaged_embedding += 1\n",
    "                \n",
    "            elif title_embedding is not None:\n",
    "                # Store only title embedding\n",
    "                G.nodes[node]['title_embedding'] = title_embedding\n",
    "                G.nodes[node]['embedding'] = title_embedding\n",
    "                acts_with_single_embedding += 1\n",
    "                \n",
    "            elif short_title_embedding is not None:\n",
    "                # Store only short title embedding\n",
    "                G.nodes[node]['title_short_embedding'] = short_title_embedding\n",
    "                G.nodes[node]['embedding'] = short_title_embedding\n",
    "                acts_with_single_embedding += 1\n",
    "                \n",
    "            else:\n",
    "                # Assign empty embedding if we know the dimension\n",
    "                if embedding_dim is not None:\n",
    "                    G.nodes[node]['embedding'] = np.zeros(embedding_dim)\n",
    "                    acts_with_empty_embedding += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            if embedding_dim is not None:\n",
    "                G.nodes[node]['embedding'] = np.zeros(embedding_dim)\n",
    "                acts_with_empty_embedding += 1\n",
    "    \n",
    "    # Print summary\n",
    "    total_acts = len(act_nodes)\n",
    "    print(\"\\nEmbedding generation summary:\")\n",
    "    print(f\"Total act nodes: {total_acts}\")\n",
    "    print(f\"Acts with averaged embeddings: {acts_with_averaged_embedding} ({(acts_with_averaged_embedding/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Acts with single embedding: {acts_with_single_embedding} ({(acts_with_single_embedding/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Acts with empty embeddings: {acts_with_empty_embedding} ({(acts_with_empty_embedding/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = generate_embeddings_for_acts(G, client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculating act embeddings...\n",
      "Found 1146 act nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing acts: 100%|██████████| 1146/1146 [00:01<00:00, 844.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding recalculation summary:\n",
      "Total act nodes: 1146\n",
      "Acts with combined embeddings (act + articles): 1009 (88.05%)\n",
      "Acts with only original embedding: 137 (11.95%)\n",
      "Acts with only articles' embedding: 0 (0.00%)\n",
      "Acts without embeddings: 0 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def recalculate_act_embeddings(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Recalculate act embeddings by averaging their current embedding with \n",
    "    the mean embedding of their article neighbors. The final result is stored\n",
    "    as the 'embedding' attribute.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): Graph with act and article embeddings\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph: Graph with updated act embeddings\n",
    "    \"\"\"\n",
    "    print(\"Recalculating act embeddings...\")\n",
    "    \n",
    "    # Get all act nodes\n",
    "    act_nodes = [node for node in G.nodes() if G.nodes[node].get('type_node') == 'act']\n",
    "    print(f\"Found {len(act_nodes)} act nodes\")\n",
    "    \n",
    "    # Track statistics\n",
    "    acts_with_both = 0\n",
    "    acts_kept_original = 0\n",
    "    acts_with_only_articles = 0\n",
    "    acts_with_empty = 0\n",
    "    \n",
    "    for act_node in tqdm(act_nodes, desc=\"Processing acts\"):\n",
    "        try:\n",
    "            # Store original act embedding if it exists\n",
    "            if 'embedding' in G.nodes[act_node]:\n",
    "                original_act_embedding = G.nodes[act_node]['embedding'].copy()  # Make a copy\n",
    "                G.nodes[act_node]['title_based_embedding'] = original_act_embedding  # Store original\n",
    "            else:\n",
    "                original_act_embedding = None\n",
    "            \n",
    "            # Get article neighbors\n",
    "            article_neighbors = [\n",
    "                neighbor for neighbor in G.neighbors(act_node)\n",
    "                if G.nodes[neighbor].get('type_node') == 'article'\n",
    "            ]\n",
    "            \n",
    "            # Collect valid article embeddings\n",
    "            article_embeddings = []\n",
    "            for article in article_neighbors:\n",
    "                embedding = G.nodes[article].get('embedding')\n",
    "                if embedding is not None:\n",
    "                    article_embeddings.append(embedding)\n",
    "            \n",
    "            # Calculate mean article embedding if we have any\n",
    "            if article_embeddings:\n",
    "                mean_article_embedding = np.mean(article_embeddings, axis=0)\n",
    "                G.nodes[act_node]['articles_mean_embedding'] = mean_article_embedding\n",
    "            else:\n",
    "                mean_article_embedding = None\n",
    "            \n",
    "            # Decide final embedding based on what we have\n",
    "            if original_act_embedding is not None and mean_article_embedding is not None:\n",
    "                # Average both embeddings and store as main embedding\n",
    "                G.nodes[act_node]['embedding'] = (original_act_embedding + mean_article_embedding) / 2\n",
    "                acts_with_both += 1\n",
    "                \n",
    "            elif original_act_embedding is not None:\n",
    "                # Keep original as main embedding\n",
    "                G.nodes[act_node]['embedding'] = original_act_embedding\n",
    "                acts_kept_original += 1\n",
    "                \n",
    "            elif mean_article_embedding is not None:\n",
    "                # Use articles' embedding as main embedding\n",
    "                G.nodes[act_node]['embedding'] = mean_article_embedding\n",
    "                acts_with_only_articles += 1\n",
    "                \n",
    "            else:\n",
    "                # No embeddings available\n",
    "                acts_with_empty += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing act {act_node}: {str(e)}\")\n",
    "            acts_with_empty += 1\n",
    "    \n",
    "    # Print summary\n",
    "    total_acts = len(act_nodes)\n",
    "    print(\"\\nEmbedding recalculation summary:\")\n",
    "    print(f\"Total act nodes: {total_acts}\")\n",
    "    print(f\"Acts with combined embeddings (act + articles): {acts_with_both} ({(acts_with_both/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Acts with only original embedding: {acts_kept_original} ({(acts_kept_original/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Acts with only articles' embedding: {acts_with_only_articles} ({(acts_with_only_articles/total_acts)*100:.2f}%)\")\n",
    "    print(f\"Acts without embeddings: {acts_with_empty} ({(acts_with_empty/total_acts)*100:.2f}%)\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = recalculate_act_embeddings(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homogenize embedding format to numpy array and save graph\n",
    "for node in G.nodes():\n",
    "    if 'embedding' in G.nodes[node] and not isinstance(G.nodes[node]['embedding'], np.ndarray):\n",
    "        G.nodes[node]['embedding'] = np.array(G.nodes[node]['embedding'])\n",
    "\n",
    "with open(os.path.join(RIZIV_data_path, 'base_hybrid_graph.pkl'), 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
