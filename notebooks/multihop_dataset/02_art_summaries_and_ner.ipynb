{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Import relevant dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pynvml\n",
    "import time\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load NER prompt template\n",
    "#ner_prompt_path = Path('../prompts') / 'entity_extraction.yaml'\n",
    "#with open(ner_prompt_path, 'r', encoding='utf-8') as file:\n",
    "#    ner_prompt_content = yaml.safe_load(file)\n",
    "#ner_prompt = ner_prompt_content['entity_extraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarization prompt template\n",
    "sum_prompt_path = Path('../prompts') / 'article_summarization.yaml'\n",
    "with open(sum_prompt_path, 'r', encoding='utf-8') as file:\n",
    "    sum_prompt_content = yaml.safe_load(file)\n",
    "sum_prompt = sum_prompt_content['article_summarization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/diomedea/.pyenv/versions/3.11.6/envs/graphsage/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.96688974, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity_group': 'LOC', 'score': 0.9996282, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer, grouped_entities=True)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "client = Client(host='http://localhost:11434')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_temperature():\n",
    "    # Run nvidia-smi to query GPU temperature\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=temperature.gpu\", \"--format=csv,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    # Parse and return temperature of GPU 0\n",
    "    return int(result.stdout.strip().split('\\n')[0])\n",
    "\n",
    "def gpu_temperature_rest_time():\n",
    "    if get_gpu_temperature() >= 80:\n",
    "        return 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text to process\n",
    "sample_text = \"\"\"\n",
    "Apple announced its new iPhone 15 on September 12, 2023. \n",
    "Tim Cook presented the event at Apple Park in Cupertino, California. \n",
    "The event was also streamed live on YouTube, where millions of viewers tuned in.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_json(text):\n",
    "#    # Search for a block that starts with '{' and ends with '}'.\n",
    "#    # The re.DOTALL flag allows the '.' to match newline characters.\n",
    "#    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "#    if match:\n",
    "#        json_str = match.group(0)\n",
    "#        try:\n",
    "#            # Try to decode the JSON string into a Python object\n",
    "#            return json.loads(json_str)\n",
    "#        except json.JSONDecodeError as e:\n",
    "#            print(\"Error decoding JSON:\", e)\n",
    "#            return None\n",
    "#    else:\n",
    "#        print(\"No JSON block found in the text.\")\n",
    "#        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_ollama_entities(article_summary, ner_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "## Prepare the messages with both system and user prompts\n",
    "#    ner_messages = [\n",
    "#        {\n",
    "#            'role': 'system',\n",
    "#            'content': ner_prompt['system_prompt']\n",
    "#        },\n",
    "#        {\n",
    "#            'role': 'user',\n",
    "#            'content': ner_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_summary)\n",
    "#        }\n",
    "#    ]\n",
    "#\n",
    "#    # Call Ollama API\n",
    "#    response = client.chat(\n",
    "#        model=model,\n",
    "#        messages=ner_messages,\n",
    "#       # options={\"temperature\":0.2}\n",
    "#    )\n",
    "#\n",
    "#    # Get the raw response content\n",
    "#    entities_json = extract_json(response['message']['content'])\n",
    "#\n",
    "#    return entities_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_summary(article_body, sum_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "    \n",
    "    # Prepare the messages with both system and user prompts\n",
    "    sum_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': sum_prompt['system_prompt']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': sum_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_body)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Ollama API\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=sum_messages,\n",
    "        options={\"temperature\":0.4},\n",
    "    )\n",
    "\n",
    "    # Get the raw response content\n",
    "    summary = response['message']['content']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple unveiled its iPhone 15 on September 12, 2023, at an event held at Apple Park in Cupertino, California, and streamed live on YouTube. Tim Cook presented the new device.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ollama_summary(sample_text, sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline graph\n",
    "with open(\"../data/MultiHop_graph_w_sem_embeddings.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to corpus file\n",
    "multihop_corpus_path = os.path.join(\"..\", \"data\", \"Multi-hop_RAG_dataset\", \"corpus.json\")\n",
    "\n",
    "# Read JSON\n",
    "with open(multihop_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Convert corpus data into df\n",
    "corpus_as_df = pd.DataFrame(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article:  251\n",
      "-----> Summary for article 251 finished...\n",
      "-----> Entity extraction for article 251 finished...\n",
      "Processing article:  252\n",
      "== Pausing code execution to cool down GPU... (82) ==\n",
      "-----> Summary for article 252 finished...\n",
      "-----> Entity extraction for article 252 finished...\n",
      "Processing article:  253\n",
      "-----> Summary for article 253 finished...\n",
      "-----> Entity extraction for article 253 finished...\n",
      "Processing article:  254\n",
      "-----> Summary for article 254 finished...\n",
      "-----> Entity extraction for article 254 finished...\n",
      "Processing article:  255\n",
      "-----> Summary for article 255 finished...\n",
      "-----> Entity extraction for article 255 finished...\n",
      "Processing article:  256\n",
      "-----> Summary for article 256 finished...\n",
      "-----> Entity extraction for article 256 finished...\n",
      "Processing article:  257\n",
      "-----> Summary for article 257 finished...\n",
      "-----> Entity extraction for article 257 finished...\n",
      "Processing article:  258\n",
      "== Pausing code execution to cool down GPU... (80) ==\n",
      "-----> Summary for article 258 finished...\n",
      "-----> Entity extraction for article 258 finished...\n",
      "Processing article:  259\n",
      "-----> Summary for article 259 finished...\n",
      "-----> Entity extraction for article 259 finished...\n",
      "Processing article:  260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Summary for article 260 finished...\n",
      "-----> Entity extraction for article 260 finished...\n",
      "Processing article:  261\n",
      "-----> Summary for article 261 finished...\n",
      "-----> Entity extraction for article 261 finished...\n",
      "Processing article:  262\n",
      "-----> Summary for article 262 finished...\n",
      "-----> Entity extraction for article 262 finished...\n",
      "Processing article:  263\n",
      "-----> Summary for article 263 finished...\n",
      "-----> Entity extraction for article 263 finished...\n",
      "Processing article:  264\n",
      "-----> Summary for article 264 finished...\n",
      "-----> Entity extraction for article 264 finished...\n",
      "Processing article:  265\n",
      "== Pausing code execution to cool down GPU... (81) ==\n",
      "-----> Summary for article 265 finished...\n",
      "-----> Entity extraction for article 265 finished...\n",
      "Processing article:  266\n",
      "-----> Summary for article 266 finished...\n",
      "-----> Entity extraction for article 266 finished...\n",
      "Processing article:  267\n",
      "-----> Summary for article 267 finished...\n",
      "-----> Entity extraction for article 267 finished...\n",
      "Processing article:  268\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store detected entities and counter variable\n",
    "entities_dict = {} \n",
    "\n",
    "# For each node in the graph...\n",
    "for node, data in G.nodes(data=True):\n",
    "\n",
    "    # If the node is of type \"article\"...\n",
    "    if (data[\"type\"] == 'article') and (node > 250):\n",
    "        \n",
    "        print(\"Processing article: \", node)\n",
    "        \n",
    "        # Retrieve article body \n",
    "        article_body = corpus_as_df.iloc[node][\"body\"]\n",
    "\n",
    "        # Check GPU temperature and wait if necessary \n",
    "        while gpu_temperature_rest_time() != 0:\n",
    "            print(f\"== Pausing code execution to cool down GPU... ({get_gpu_temperature()}) ==\")\n",
    "            time.sleep(gpu_temperature_rest_time())\n",
    "\n",
    "        # Generate LLM summary of article for entity extraction \n",
    "        llm_summary = get_ollama_summary(article_body, sum_prompt)\n",
    "        print(f\"-----> Summary for article {node} finished...\")\n",
    "\n",
    "        ## Initialize entity extraction loop tracking variables \n",
    "        #json_entities = None\n",
    "        #while_count = 0\n",
    "\n",
    "        ## While we don't have a valid Json output for entities... \n",
    "        #while json_entities is None:\n",
    "\n",
    "            ## Check GPU temperature and wait if necessary \n",
    "            #while gpu_temperature_rest_time() != 0:\n",
    "            #    print(f\"== Pausing code execution to cool down GPU... ({get_gpu_temperature()}) ==\")\n",
    "            #    time.sleep(gpu_temperature_rest_time())\n",
    "\n",
    "            ## Extract entities \n",
    "            #json_entities = get_ollama_entities(llm_summary, ner_prompt)\n",
    "            #while_count += 1\n",
    "\n",
    "            ## If after three attempts the output is not valid... \n",
    "            #if while_count >= 3:\n",
    "                ## Exit loop and skip  \n",
    "                #print(\"----------> JSON not extracted for: \", node)\n",
    "                #break\n",
    "\n",
    "        ner_results = nlp(llm_summary)\n",
    "            \n",
    "        print(f\"-----> Entity extraction for article {node} finished...\")\n",
    "\n",
    "        # Include entities and summary in \"entities_dict\" \n",
    "        entities_dict[node] ={\"entities\":ner_results, \"summary\":llm_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/extracted_entities_XXX.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entities_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_chunk_0', '0_chunk_1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Identify entity citations in chunks\n",
    "#\n",
    "## for each article in the multi-hop dataset\n",
    "#for article_id in range(610):\n",
    "#\n",
    "#    # identify all \"chunk\" nodes derived from a given article \n",
    "#    prefix = f\"{article_id}_chunk\"\n",
    "#    matching_nodes = [node for node in G.nodes if str(node).startswith(prefix)]\n",
    "#\n",
    "#    # for each chunk node \n",
    "#    for node in matching_nodes:\n",
    "#\n",
    "#        # for each entity found in an article... \n",
    "#        for entity in entities_dict[article_id]['entities']['entities']:\n",
    "#\n",
    "#            # Apply \"setdefault\" method with \"appears_in\" list of entity\n",
    "#            entity.setdefault('appears_in', [])\n",
    "#\n",
    "#            # If the entity appears in the considered chunk...\n",
    "#            if entity['name'] in G.nodes[node][\"text\"]:\n",
    "#\n",
    "#                # Add chunk node id to the entity's 'appears_in' list \n",
    "#                entity['appears_in'].append(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temperature = get_gpu_temeprature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"../data/extracted_entities_B.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(entities_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
