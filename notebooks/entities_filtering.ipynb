{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities list A (0 - 249)    \n",
    "#with open('../data/extracted_entities_250.pkl', 'rb') as f:\n",
    "#    entities_250 = pickle.load(f)\n",
    "#\n",
    "# Load entities list B (251 - 608)\n",
    "#with open('../data/extracted_entities_608.pkl', 'rb') as f:\n",
    "#    entities_608 = pickle.load(f)\n",
    "#\n",
    "# Merge both lists\n",
    "#entities = {**entities_250, **entities_608}\n",
    "#\n",
    "## Save the merged dictionary\n",
    "#with open('../data/multihop_dataset_entities.pkl', 'wb') as f:\n",
    "#    pickle.dump(entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary\n",
    "with open('../data/multihop_dataset_raw_entities.pkl', 'rb') as f:\n",
    "    entities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial counters\n",
    "total_entities = 0  # All entities (including duplicates)\n",
    "entity_appearances = {}  # Dictionary to track entity appearances\n",
    "\n",
    "#---------------\n",
    "# 1. Unify list of entities, unifying duplicates intra and inter article, and including reference to articles where entity is referenced\n",
    "#---------------\n",
    "# Process each article\n",
    "for article_id, article_data in entities.items():\n",
    "    # Count all entities in this article\n",
    "    entities_list = article_data[\"entities\"]\n",
    "    total_entities += len(entities_list)\n",
    "    \n",
    "    # Process each entity in the article\n",
    "    for entity in entities_list:\n",
    "        # Create a tuple key with the entity and its type\n",
    "        entity_key = (entity[\"word\"], entity[\"entity_group\"])\n",
    "        \n",
    "        # If entity exists, add article_id to its set of articles\n",
    "        # If it doesn't exist, create new entry with a set containing this article_id\n",
    "        if entity_key in entity_appearances:\n",
    "            entity_appearances[entity_key].add(article_id)\n",
    "        else:\n",
    "            entity_appearances[entity_key] = {article_id}\n",
    "\n",
    "# Convert dictionary to list of tuples (entity, type, set_of_articles)\n",
    "unique_entities_with_articles = [\n",
    "    (entity, entity_type, article_ids)\n",
    "    for (entity, entity_type), article_ids in entity_appearances.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILTERING RESULTS ===\n",
      "1. Original entities: 7699\n",
      "2. After removing hashtags: 7452\n",
      "3. After removing 'MISC' type: 3254\n",
      "4. After removing single-article entities: 743\n"
     ]
    }
   ],
   "source": [
    "#---------------\n",
    "# 2. Filter entities: Configuration flags\n",
    "#---------------\n",
    "FILTER_HASHTAGS = True      # Enable/disable hashtag filtering\n",
    "FILTER_MISC_TYPE = True    # Enable/disable MISC type filtering\n",
    "FILTER_SINGLE_ARTICLE = True  # Enable/disable single-article entity filtering\n",
    "\n",
    "#---------------\n",
    "# 2.1 Filter out hashtag entities\n",
    "#---------------\n",
    "if FILTER_HASHTAGS:\n",
    "    no_hashtag_entities = [\n",
    "        (entity, entity_type, article_ids) \n",
    "        for entity, entity_type, article_ids in unique_entities_with_articles \n",
    "        if '#' not in entity\n",
    "    ]\n",
    "else:\n",
    "    no_hashtag_entities = unique_entities_with_articles\n",
    "\n",
    "#---------------\n",
    "# 2.2 Filter out 'MISC' type entities\n",
    "#---------------\n",
    "if FILTER_MISC_TYPE:\n",
    "    no_misc_entities = [\n",
    "        (entity, entity_type, article_ids) \n",
    "        for entity, entity_type, article_ids in no_hashtag_entities \n",
    "        if entity_type != 'MISC'\n",
    "    ]\n",
    "else:\n",
    "    no_misc_entities = no_hashtag_entities\n",
    "\n",
    "#---------------\n",
    "# 2.3 Filter out entities that appear in only one article\n",
    "#---------------\n",
    "if FILTER_SINGLE_ARTICLE:\n",
    "    filtered_entities = [\n",
    "        (entity, entity_type, article_ids) \n",
    "        for entity, entity_type, article_ids in no_misc_entities \n",
    "        if len(article_ids) > 1\n",
    "    ]\n",
    "else:\n",
    "    filtered_entities = no_misc_entities\n",
    "\n",
    "# Print filtering results\n",
    "print(\"\\n=== FILTERING RESULTS ===\")\n",
    "print(f\"1. Original entities: {len(unique_entities_with_articles)}\")\n",
    "if FILTER_HASHTAGS:\n",
    "    print(f\"2. After removing hashtags: {len(no_hashtag_entities)}\")\n",
    "if FILTER_MISC_TYPE:\n",
    "    print(f\"3. After removing 'MISC' type: {len(no_misc_entities)}\")\n",
    "if FILTER_SINGLE_ARTICLE:\n",
    "    print(f\"4. After removing single-article entities: {len(filtered_entities)}\")\n",
    "print(f\"\\nFinal number of entities: {len(filtered_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered entities dictionary\n",
    "with open('../data/multihop_dataset_filtered_entities.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENTITY ANALYSIS SUMMARY ===\n",
      "1. Total entities found (including duplicates): 13852\n",
      "2. Total unique entities (without duplicates): 7699\n",
      "3. Total entities after filtering: 743\n",
      "\n",
      "4. Examples of unique entities and their appearances (first 5):\n",
      "   - Amazon (ORG) appears in 35 articles: [0, 1, 2]...\n",
      "   - Cyber Monday (MISC) appears in 4 articles: [0, 91, 222]...\n",
      "   - Black Friday (MISC) appears in 9 articles: [0, 221, 222]...\n",
      "   - Echo (MISC) appears in 4 articles: [0, 188, 222]...\n",
      "   - Fire TV (MISC) appears in 1 articles: [0]...\n",
      "\n",
      "5. Distribution by entity type after filtering:\n",
      "   - LOC: 90 entities\n",
      "   - ORG: 314 entities\n",
      "   - PER: 339 entities\n",
      "\n",
      "6. Overall average appearances per entity: 3.64 articles\n",
      "\n",
      "7. Average appearances by entity type:\n",
      "   - LOC (90 entities): 4.81 articles per entity\n",
      "   - ORG (314 entities): 4.08 articles per entity\n",
      "   - PER (339 entities): 2.91 articles per entity\n"
     ]
    }
   ],
   "source": [
    "# Summary final set of entities\n",
    "\n",
    "entity_type_counts = {}\n",
    "for _, entity_type, _ in filtered_entities:\n",
    "    entity_type_counts[entity_type] = entity_type_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\n=== ENTITY ANALYSIS SUMMARY ===\")\n",
    "print(f\"1. Total entities found (including duplicates): {total_entities}\")\n",
    "print(f\"2. Total unique entities (without duplicates): {len(unique_entities_with_articles)}\")\n",
    "print(f\"3. Total entities after filtering: {len(filtered_entities)}\")\n",
    "\n",
    "print(\"\\n4. Examples of unique entities and their appearances (first 5):\")\n",
    "for entity, entity_type, article_ids in unique_entities_with_articles[:5]:\n",
    "    print(f\"   - {entity} ({entity_type}) appears in {len(article_ids)} articles: {sorted(article_ids)[:3]}...\")\n",
    "\n",
    "print(\"\\n5. Distribution by entity type after filtering:\")\n",
    "for entity_type, count in sorted(entity_type_counts.items()):\n",
    "    print(f\"   - {entity_type}: {count} entities\")\n",
    "\n",
    "# Calculate overall average of appearances\n",
    "total_appearances = sum(len(article_ids) for _, _, article_ids in filtered_entities)\n",
    "average_appearances = total_appearances / len(filtered_entities)\n",
    "\n",
    "# Calculate averages by type\n",
    "type_stats = {}\n",
    "for _, entity_type, article_ids in filtered_entities:\n",
    "    if entity_type not in type_stats:\n",
    "        type_stats[entity_type] = {\n",
    "            'total_appearances': 0,\n",
    "            'entity_count': 0\n",
    "        }\n",
    "    type_stats[entity_type]['total_appearances'] += len(article_ids)\n",
    "    type_stats[entity_type]['entity_count'] += 1\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n6. Overall average appearances per entity: {average_appearances:.2f} articles\")\n",
    "print(\"\\n7. Average appearances by entity type:\")\n",
    "for entity_type, stats in sorted(type_stats.items()):\n",
    "    avg = stats['total_appearances'] / stats['entity_count']\n",
    "    count = stats['entity_count']\n",
    "    print(f\"   - {entity_type} ({count} entities): {avg:.2f} articles per entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
