{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pynvml\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "# Load NER prompt template\n",
    "ner_prompt_path = Path('../prompts') / 'entity_extraction.yaml'\n",
    "with open(ner_prompt_path, 'r', encoding='utf-8') as file:\n",
    "    ner_prompt_content = yaml.safe_load(file)\n",
    "ner_prompt = ner_prompt_content['entity_extraction']\n",
    "\n",
    "# Load summarization prompt template\n",
    "sum_prompt_path = Path('../prompts') / 'article_summarization.yaml'\n",
    "with open(sum_prompt_path, 'r', encoding='utf-8') as file:\n",
    "    sum_prompt_content = yaml.safe_load(file)\n",
    "sum_prompt = sum_prompt_content['article_summarization']\n",
    "\n",
    "# Example text to process\n",
    "sample_text = \"\"\"\n",
    "Apple announced its new iPhone 15 on September 12, 2023. \n",
    "Tim Cook presented the event at Apple Park in Cupertino, California. \n",
    "The event was also streamed live on YouTube, where millions of viewers tuned in.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_temperature():\n",
    "    # Run nvidia-smi to query GPU temperature\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=temperature.gpu\", \"--format=csv,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    # Parse and return temperature of GPU 0\n",
    "    return int(result.stdout.strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_temperature_rest_time():\n",
    "    if get_gpu_temperature() >= 80:\n",
    "        return 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # Search for a block that starts with '{' and ends with '}'.\n",
    "    # The re.DOTALL flag allows the '.' to match newline characters.\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            # Try to decode the JSON string into a Python object\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error decoding JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON block found in the text.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_summary(article_body, sum_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "# Prepare the messages with both system and user prompts\n",
    "    sum_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': sum_prompt['system_prompt']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': sum_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_body)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Ollama API\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=sum_messages,\n",
    "        options={\"temperature\":0.4}\n",
    "    )\n",
    "\n",
    "    # Get the raw response content\n",
    "    summary = response['message']['content']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_entities(article_summary, ner_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "# Prepare the messages with both system and user prompts\n",
    "    ner_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': ner_prompt['system_prompt']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': ner_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_summary)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Ollama API\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=ner_messages,\n",
    "       # options={\"temperature\":0.2}\n",
    "    )\n",
    "\n",
    "    # Get the raw response content\n",
    "    entities_json = extract_json(response['message']['content'])\n",
    "\n",
    "    return entities_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline graph\n",
    "with open(\"../data/MultiHop_graph_w_sem_embeddings.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to corpus file\n",
    "multihop_corpus_path = os.path.join(\"..\", \"data\", \"Multi-hop_RAG_dataset\", \"corpus.json\")\n",
    "\n",
    "# Read JSON\n",
    "with open(multihop_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Convert corpus data into df\n",
    "corpus_as_df = pd.DataFrame(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article:  0\n",
      "-----> Summary for article 0 finished...\n",
      "-----> Entity extraction for article 0 finished...\n",
      "Processing article:  1\n",
      "-----> Summary for article 1 finished...\n",
      "-----> Entity extraction for article 1 finished...\n",
      "Processing article:  2\n",
      "== Pausing code execution to cool down GPU... (83) ==\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store detected entities and counter variable\n",
    "entities_dict = {} \n",
    "\n",
    "# For each node in the graph...\n",
    "for node, data in G.nodes(data=True):\n",
    "\n",
    "    # If the node is of type \"article\"...\n",
    "    if data[\"type\"] == 'article':\n",
    "        \n",
    "        print(\"Processing article: \", node)\n",
    "        \n",
    "        # Retrieve article body \n",
    "        article_body = corpus_as_df.iloc[node][\"body\"]\n",
    "\n",
    "        # Check GPU temperature and wait if necessary \n",
    "        while gpu_temperature_rest_time() != 0:\n",
    "            print(f\"== Pausing code execution to cool down GPU... ({get_gpu_temperature()}) ==\")\n",
    "            time.sleep(gpu_temperature_rest_time())\n",
    "\n",
    "        # Generate LLM summary of article for entity extraction \n",
    "        llm_summary = get_ollama_summary(article_body, sum_prompt)\n",
    "        print(f\"-----> Summary for article {node} finished...\")\n",
    "\n",
    "       # Initialize entity extraction loop tracking variables \n",
    "        json_entities = None\n",
    "        while_count = 0\n",
    "\n",
    "        # While we don't have a valid Json output for entities... \n",
    "        while json_entities is None:\n",
    "\n",
    "            # Check GPU temperature and wait if necessary \n",
    "            while gpu_temperature_rest_time() != 0:\n",
    "                print(f\"== Pausing code execution to cool down GPU... ({get_gpu_temperature()}) ==\")\n",
    "                time.sleep(gpu_temperature_rest_time())\n",
    "\n",
    "            # Extract entities \n",
    "            json_entities = get_ollama_entities(llm_summary, ner_prompt)\n",
    "            while_count += 1\n",
    "\n",
    "            # If after three attempts the output is not valid... \n",
    "            if while_count >= 3:\n",
    "                # Exit loop and skip  \n",
    "                print(\"----------> JSON not extracted for: \", node)\n",
    "                break\n",
    "        print(f\"-----> Entity extraction for article {node} finished...\")\n",
    "\n",
    "        # Include entities and summary in \"entities_dict\" \n",
    "        entities_dict[node] ={\"entities\":json_entities, \"summary\":llm_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'entities': {'entities': [{'type': 'Organization',\n",
       "     'name': 'Amazon',\n",
       "     'sentences': ['Amazon is holding an 11-day shopping event, beginning November 17th and continuing through Cyber Monday, November 27th, with both Black Friday and Cyber Monday deals available.',\n",
       "      'with a $50 Amazon credit.']},\n",
       "    {'type': 'Date',\n",
       "     'name': 'November 17th',\n",
       "     'sentences': ['Amazon is holding an 11-day shopping event, beginning November 17th and continuing through Cyber Monday, November 27th, with both Black Friday and Cyber Monday deals available.']},\n",
       "    {'type': 'Date',\n",
       "     'name': 'November 27th',\n",
       "     'sentences': ['Amazon is holding an 11-day shopping event, beginning November 17th and continuing through Cyber Monday, November 27th, with both Black Friday and Cyber Monday deals available.']},\n",
       "    {'type': 'Organization',\n",
       "     'name': 'Apple',\n",
       "     'sentences': ['The article highlights deals across numerous categories including Amazon devices (Echo, Fire TV, Kindle), Apple products, TVs, laptops, headphones, tablets, gaming, speakers, vacuums, kitchen appliances, smart home devices, fitness trackers, beauty tech, drones, cameras, Lego, and gift cards.']},\n",
       "    {'type': 'Organization',\n",
       "     'name': 'Microsoft',\n",
       "     'sentences': ['Specific deals mentioned include the Echo Show for under $40, the 10th generation 64GB iPad for $349, a 65-inch Fire TV at a record low price, the Microsoft Surface Laptop Go 3 for $599.99, Bose QuietComfort 45 headphones for $199, and the Meta Quest 2 for $249 with a $50 Amazon credit.']},\n",
       "    {'type': 'Organization',\n",
       "     'name': 'Bose',\n",
       "     'sentences': ['Specific deals mentioned include the Echo Show for under $40, the 10th generation 64GB iPad for $349, a 65-inch Fire TV at a record low price, the Microsoft Surface Laptop Go 3 for $599.99, Bose QuietComfort 45 headphones for $199, and the Meta Quest 2 for $249 with a $50 Amazon credit.']},\n",
       "    {'type': 'Organization',\n",
       "     'name': 'Meta',\n",
       "     'sentences': ['Specific deals mentioned include the Echo Show for under $40, the 10th generation 64GB iPad for $349, a 65-inch Fire TV at a record low price, the Microsoft Surface Laptop Go 3 for $599.99, Bose QuietComfort 45 headphones for $199, and the Meta Quest 2 for $249 with a $50 Amazon credit.']}]},\n",
       "  'summary': 'Amazon is holding an 11-day shopping event, beginning November 17th and continuing through Cyber Monday, November 27th, with both Black Friday and Cyber Monday deals available. The article highlights deals across numerous categories including Amazon devices (Echo, Fire TV, Kindle), Apple products, TVs, laptops, headphones, tablets, gaming, speakers, vacuums, kitchen appliances, smart home devices, fitness trackers, beauty tech, drones, cameras, Lego, and gift cards. \\n\\nSpecific deals mentioned include the Echo Show for under $40, the 10th generation 64GB iPad for $349, a 65-inch Fire TV at a record low price, the Microsoft Surface Laptop Go 3 for $599.99, Bose QuietComfort 45 headphones for $199, and the Meta Quest 2 for $249 with a $50 Amazon credit. Deals are frequently updated, with new additions marked with a âœ¨, all-time low prices marked with a ðŸ”¥, and Prime member exclusives marked with a ðŸ“¨. Deals that have sold out or expired are marked with a strikeout.'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/extracted_entities_A.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entities_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_chunk_0', '0_chunk_1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify entity citations in chunks\n",
    "\n",
    "# for each article in the multi-hop dataset\n",
    "for article_id in range(610):\n",
    "\n",
    "    # identify all \"chunk\" nodes derived from a given article \n",
    "    prefix = f\"{article_id}_chunk\"\n",
    "    matching_nodes = [node for node in G.nodes if str(node).startswith(prefix)]\n",
    "\n",
    "    # for each chunk node \n",
    "    for node in matching_nodes:\n",
    "\n",
    "        # for each entity found in an article... \n",
    "        for entity in entities_dict[article_id]['entities']['entities']:\n",
    "\n",
    "            # Apply \"setdefault\" method with \"appears_in\" list of entity\n",
    "            entity.setdefault('appears_in', [])\n",
    "\n",
    "            # If the entity appears in the considered chunk...\n",
    "            if entity['name'] in G.nodes[node][\"text\"]:\n",
    "\n",
    "                # Add chunk node id to the entity's 'appears_in' list \n",
    "                entity['appears_in'].append(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temperature = get_gpu_temeprature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/extracted_entities_B.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entities_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
