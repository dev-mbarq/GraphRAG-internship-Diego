{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from ollama import Client\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pynvml\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "# Load NER prompt template\n",
    "ner_prompt_path = Path('../prompts') / 'entity_extraction.yaml'\n",
    "with open(ner_prompt_path, 'r', encoding='utf-8') as file:\n",
    "    ner_prompt_content = yaml.safe_load(file)\n",
    "ner_prompt = ner_prompt_content['entity_extraction']\n",
    "\n",
    "# Load summarization prompt template\n",
    "sum_prompt_path = Path('../prompts') / 'article_summarization.yaml'\n",
    "with open(sum_prompt_path, 'r', encoding='utf-8') as file:\n",
    "    sum_prompt_content = yaml.safe_load(file)\n",
    "sum_prompt = sum_prompt_content['article_summarization']\n",
    "\n",
    "# Example text to process\n",
    "sample_text = \"\"\"\n",
    "Apple announced its new iPhone 15 on September 12, 2023. \n",
    "Tim Cook presented the event at Apple Park in Cupertino, California. \n",
    "The event was also streamed live on YouTube, where millions of viewers tuned in.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_temperature():\n",
    "    # Run nvidia-smi to query GPU temperature\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=temperature.gpu\", \"--format=csv,noheader\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    # Parse and return temperature of GPU 0\n",
    "    return int(result.stdout.strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_temperature_rest_time():\n",
    "    if get_gpu_temperature() >= 80:\n",
    "        return 60\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # Search for a block that starts with '{' and ends with '}'.\n",
    "    # The re.DOTALL flag allows the '.' to match newline characters.\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            # Try to decode the JSON string into a Python object\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error decoding JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON block found in the text.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_summary(article_body, sum_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "# Prepare the messages with both system and user prompts\n",
    "    sum_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': sum_prompt['system_prompt']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': sum_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_body)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Ollama API\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=sum_messages,\n",
    "        options={\"temperature\":0.4}\n",
    "    )\n",
    "\n",
    "    # Get the raw response content\n",
    "    summary = response['message']['content']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_entities(article_summary, ner_prompt, model=\"gemma3:27b-it-q8_0\"):\n",
    "# Prepare the messages with both system and user prompts\n",
    "    ner_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': ner_prompt['system_prompt']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': ner_prompt['user_prompt_template'].replace(\"{text_to_process}\", article_summary)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Ollama API\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=ner_messages,\n",
    "        options={\"temperature\":0.2}\n",
    "    )\n",
    "\n",
    "    # Get the raw response content\n",
    "    entities_json = extract_json(response['message']['content'])\n",
    "\n",
    "    return entities_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline graph\n",
    "with open(\"../data/MultiHop_graph_w_sem_embeddings.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to corpus file\n",
    "multihop_corpus_path = os.path.join(\"..\", \"data\", \"Multi-hop_RAG_dataset\", \"corpus.json\")\n",
    "\n",
    "# Read JSON\n",
    "with open(multihop_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Convert corpus data into df\n",
    "corpus_as_df = pd.DataFrame(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article:  0\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store detected entities and counter variable\n",
    "entities_dict = {} \n",
    "\n",
    "# For each node in the graph...\n",
    "for node, data in G.nodes(data=True):\n",
    "\n",
    "    # If the node is of type \"article\"...\n",
    "    if data[\"type\"] == 'article':\n",
    "        \n",
    "        print(\"Processing article: \", node)\n",
    "        \n",
    "        # Retrieve article body \n",
    "        article_body = corpus_as_df.iloc[node][\"body\"]\n",
    "\n",
    "        # Check GPU temperature and wait if necessary \n",
    "        while gpu_temperature_rest_time() != 0:\n",
    "            time.sleep(gpu_temperature_rest_time())\n",
    "            print(f\"Pausing code execution to cool down GPU... ({get_gpu_temperature()})\")\n",
    "\n",
    "        # Generate LLM summary of article for entity extraction \n",
    "        llm_summary = get_ollama_summary(article_body, sum_prompt)\n",
    "\n",
    "       # Initialize entity extraction loop tracking variables \n",
    "        json_entities = None\n",
    "        while_count = 0\n",
    "\n",
    "        # While we don't have a valid Json output for entities... \n",
    "        while json_entities is None:\n",
    "\n",
    "            # Check GPU temperature and wait if necessary \n",
    "            while gpu_temperature_rest_time() != 0:\n",
    "                time.sleep(gpu_temperature_rest_time())\n",
    "                print(f\"Pausing code execution to cool down GPU... ({get_gpu_temperature()})\")\n",
    "\n",
    "            # Extract entities \n",
    "            json_entities = get_ollama_entities(llm_summary, ner_prompt)\n",
    "            while_count += 1\n",
    "\n",
    "            # If after three attempts the output is not valid... \n",
    "            if while_count == 3:\n",
    "                # Exit loop and skip  \n",
    "                print(\"JSON not extracted for: \", node)\n",
    "                break\n",
    "\n",
    "        # Include entities and summary in \"entities_dict\" \n",
    "        entities_dict[node] ={\"entities\":json_entities, \"summary\":llm_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_chunk_0', '0_chunk_1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify entity citations in chunks\n",
    "\n",
    "# for each article in the multi-hop dataset\n",
    "for article_id in range(610):\n",
    "\n",
    "    # identify all \"chunk\" nodes derived from a given article \n",
    "    prefix = f\"{article_id}_chunk\"\n",
    "    matching_nodes = [node for node in G.nodes if str(node).startswith(prefix)]\n",
    "\n",
    "    # for each chunk node \n",
    "    for node in matching_nodes:\n",
    "\n",
    "        # for each entity found in an article... \n",
    "        for entity in entities_dict[article_id]['entities']['entities']:\n",
    "\n",
    "            # Apply \"setdefault\" method with \"appears_in\" list of entity\n",
    "            entity.setdefault('appears_in', [])\n",
    "\n",
    "            # If the entity appears in the considered chunk...\n",
    "            if entity['name'] in G.nodes[node][\"text\"]:\n",
    "\n",
    "                # Add chunk node id to the entity's 'appears_in' list \n",
    "                entity['appears_in'].append(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temperature = get_gpu_temeprature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/extracted_entities.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entities_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
