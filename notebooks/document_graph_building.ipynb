{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import gremlin_python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to corpus file\n",
    "multihop_corpus_path = os.path.join(\"..\", \"data\", \"Multi-hop_RAG_dataset\", \"corpus.json\")\n",
    "\n",
    "# Read JSON\n",
    "with open(multihop_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Convert corpus data into df\n",
    "corpus_as_df = pd.DataFrame(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>published_at</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200+ of the best deals from Amazon's Cyber Mon...</td>\n",
       "      <td>None</td>\n",
       "      <td>Mashable</td>\n",
       "      <td>2023-11-27T08:45:59+00:00</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>https://mashable.com/article/cyber-monday-deal...</td>\n",
       "      <td>Table of Contents Table of Contents Echo, Fire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASX set to drop as Wall Street’s September slu...</td>\n",
       "      <td>Stan Choe</td>\n",
       "      <td>The Sydney Morning Herald</td>\n",
       "      <td>2023-09-26T19:11:30+00:00</td>\n",
       "      <td>business</td>\n",
       "      <td>https://www.smh.com.au/business/markets/asx-se...</td>\n",
       "      <td>ETF provider Betashares, which manages $30 bil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon sellers sound off on the FTC's 'long-ov...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cnbc | World Business News Leader</td>\n",
       "      <td>2023-10-06T21:31:00+00:00</td>\n",
       "      <td>business</td>\n",
       "      <td>https://www.cnbc.com/2023/10/06/amazon-sellers...</td>\n",
       "      <td>A worker sorts out parcels in the outbound doc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christmas Day preview: 49ers, Ravens square of...</td>\n",
       "      <td>Colum Dell, Yardbarker</td>\n",
       "      <td>Yardbarker</td>\n",
       "      <td>2023-12-24T23:34:39+00:00</td>\n",
       "      <td>sports</td>\n",
       "      <td>https://www.yardbarker.com/nfl/articles/christ...</td>\n",
       "      <td>Christmas Day isn't just for the NBA, as the N...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raiders vs. Lions live score, updates, highlig...</td>\n",
       "      <td>Dan Treacy</td>\n",
       "      <td>Sporting News</td>\n",
       "      <td>2023-10-30T22:20:03+00:00</td>\n",
       "      <td>sports</td>\n",
       "      <td>https://www.sportingnews.com/us/nfl/news/raide...</td>\n",
       "      <td>The Lions just needed to get themselves back i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  author  \\\n",
       "0  200+ of the best deals from Amazon's Cyber Mon...                    None   \n",
       "1  ASX set to drop as Wall Street’s September slu...               Stan Choe   \n",
       "2  Amazon sellers sound off on the FTC's 'long-ov...                    None   \n",
       "3  Christmas Day preview: 49ers, Ravens square of...  Colum Dell, Yardbarker   \n",
       "4  Raiders vs. Lions live score, updates, highlig...              Dan Treacy   \n",
       "\n",
       "                              source               published_at  \\\n",
       "0                           Mashable  2023-11-27T08:45:59+00:00   \n",
       "1          The Sydney Morning Herald  2023-09-26T19:11:30+00:00   \n",
       "2  Cnbc | World Business News Leader  2023-10-06T21:31:00+00:00   \n",
       "3                         Yardbarker  2023-12-24T23:34:39+00:00   \n",
       "4                      Sporting News  2023-10-30T22:20:03+00:00   \n",
       "\n",
       "        category                                                url  \\\n",
       "0  entertainment  https://mashable.com/article/cyber-monday-deal...   \n",
       "1       business  https://www.smh.com.au/business/markets/asx-se...   \n",
       "2       business  https://www.cnbc.com/2023/10/06/amazon-sellers...   \n",
       "3         sports  https://www.yardbarker.com/nfl/articles/christ...   \n",
       "4         sports  https://www.sportingnews.com/us/nfl/news/raide...   \n",
       "\n",
       "                                                body  article_id  \n",
       "0  Table of Contents Table of Contents Echo, Fire...           0  \n",
       "1  ETF provider Betashares, which manages $30 bil...           1  \n",
       "2  A worker sorts out parcels in the outbound doc...           2  \n",
       "3  Christmas Day isn't just for the NBA, as the N...           3  \n",
       "4  The Lions just needed to get themselves back i...           4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new feature to act as a unique identifier for each article\n",
    "corpus_as_df[\"article_id\"] = corpus_as_df.index.copy()\n",
    "corpus_as_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define sets to store unique authors, sources, and categories and avoid duplicates\n",
    "authors = set()\n",
    "sources = set()\n",
    "categories = set()\n",
    "\n",
    "# Iterate over rows in corpus and add nodes and edges to graph\n",
    "for i, row in enumerate(corpus_as_df.iterrows()):\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "    # Extract data from row\n",
    "    article_id = row[\"article_id\"]\n",
    "\n",
    "    # Add article node\n",
    "    G.add_node(article_id, title=row[\"title\"], type=\"article\")\n",
    "\n",
    "    # If author is not in set, add author node and edges\n",
    "    author_id = row[\"author\"]\n",
    "    if author_id not in authors:\n",
    "        authors.add(author_id)\n",
    "        G.add_node(author_id, type=\"author\")\n",
    "\n",
    "    G.add_edge(article_id, author_id, relation=\"WRITTEN_BY\")  # (Ar -> Au)\n",
    "    G.add_edge(author_id, article_id, relation=\"AUTHORED\")    # (Au -> Ar)\n",
    "\n",
    "    # If author is not in set, add author node and edges\n",
    "    source_id = row[\"source\"]\n",
    "    if source_id not in sources:\n",
    "        sources.add(source_id)\n",
    "        G.add_node(source_id, type=\"source\")\n",
    "\n",
    "    G.add_edge(article_id, source_id, relation=\"PUBLISHED_IN\")  # (Ar -> So)\n",
    "    G.add_edge(source_id, article_id, relation=\"PUBLISHES\")     # (So -> Ar)\n",
    "\n",
    "    # If author is not in set, add author node and edges\n",
    "    category_id = row[\"category\"]\n",
    "    if category_id not in categories:\n",
    "        categories.add(author_id)\n",
    "        G.add_node(category_id, type=\"category\")\n",
    "\n",
    "    G.add_edge(article_id, category_id, relation=\"CLASSIFIED_WITHIN\")  # (Ar -> Ca)\n",
    "    G.add_edge(category_id, article_id, relation=\"COVERED_IN\")         # (Ca -> Ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m text_splitter = RecursiveCharacterTextSplitter(\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Set a really small chunk size, just to show.\u001b[39;00m\n\u001b[32m      3\u001b[39m     chunk_size=\u001b[32m100\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     is_separator_regex=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m texts = \u001b[43mtext_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(texts[\u001b[32m0\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(texts[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/langchain_text_splitters/base.py:79\u001b[39m, in \u001b[36mTextSplitter.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m     77\u001b[39m index = \u001b[32m0\u001b[39m\n\u001b[32m     78\u001b[39m previous_chunk_len = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     80\u001b[39m     metadata = copy.deepcopy(_metadatas[i])\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/langchain_text_splitters/character.py:126\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Split the input text into smaller chunks based on predefined separators.\u001b[39;00m\n\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m \u001b[33;03m        List[str]: A list of text chunks obtained after splitting.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/langchain_text_splitters/character.py:88\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter._split_text\u001b[39m\u001b[34m(self, text, separators)\u001b[39m\n\u001b[32m     86\u001b[39m     separator = _s\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     89\u001b[39m     separator = _s\n\u001b[32m     90\u001b[39m     new_separators = separators[i + \u001b[32m1\u001b[39m :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/re/__init__.py:177\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([corpus])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
