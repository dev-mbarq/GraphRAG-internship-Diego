{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "# Add \"src\" path to Python path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Import custom graph formatting function\n",
    "from graph_formatting_utils import format_graph_for_graphsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA L40S\n",
      "CUDA Device Count: 1\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA status\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline graph\n",
    "with open(\"../data/MultiHop_graph_w_sem_embeddings.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_G = format_graph_for_graphsage(G, embedding_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for initial node embeddings\n",
    "for node in cleaned_G.nodes():\n",
    "    emb = cleaned_G.nodes[node][\"embedding\"]\n",
    "    if emb is None:\n",
    "        print(f\"[ERROR] Node {node} is still missing embedding!\")\n",
    "    elif not isinstance(emb, torch.Tensor):\n",
    "        print(f\"[ERROR] Node {node} embedding is not a torch.Tensor!\")\n",
    "    elif emb.shape != (1024,):\n",
    "        print(f\"[ERROR] Node {node} has shape {emb.shape} != ({1024},)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NetworkX graph to a PyTorch Geometric Data object\n",
    "data = from_networkx(cleaned_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the graph is undirected\n",
    "data.edge_index = to_undirected(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data attribute \"x\" containing the embeddings of each node complying with the PyTorch Geometric API\n",
    "data.x = data.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphSAGE model class\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GraphSAGE model for node representation learning in graphs.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    in_channels : int\n",
    "        The dimensionality of input node features (e.g., embedding size).\n",
    "    hidden_channels : int\n",
    "        The dimensionality of hidden layers in the GraphSAGE model.\n",
    "    out_channels : int\n",
    "        The dimensionality of the output node representations.\n",
    "    num_layers : int, optional\n",
    "        The number of GraphSAGE layers (default is 2).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x, edge_index):\n",
    "        Performs forward propagation through the GraphSAGE layers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        The learned node embeddings of shape (num_nodes, out_channels).\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The first layer transforms input embeddings into a hidden representation.\n",
    "    - Intermediate layers apply non-linear transformations (`ReLU` activation).\n",
    "    - The final layer outputs node embeddings without activation.\n",
    "    - Uses `SAGEConv` layers from PyTorch Geometric (`torch_geometric.nn`).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First GraphSAGE layer: input (embeddings) → hidden layer\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        \n",
    "        # Intermediate layers (if num_layers > 2)\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Last GraphSAGE layer: hidden layer → final embedding\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:  # Intermediate layers\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)  # ReLU activation\n",
    "        x = self.convs[-1](x, edge_index)  # Last layer (no activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GraphSAGE model\n",
    "model = GraphSAGE(\n",
    "    in_channels=1024,   # Input features (BGE-M3 embeddings)\n",
    "    hidden_channels=512,  # First hidden layer (alto para máxima capacidad)\n",
    "    out_channels=256,   # Output embeddings (más ricos)\n",
    "    num_layers=2        # Mantenemos 2 capas (2 hops)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diomedea/.pyenv/versions/3.11.6/envs/graphsage/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the NeighborLoader for mini-batch training\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[25, 15],  # 25 neighbors for the first layer, 15 for the second\n",
    "    batch_size=512,  # Batch size\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set device for model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Check model device\n",
    "print(\"Model in:\", next(model.parameters()).device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define scaler\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_loss(z, edge_index, num_neg_samples=5):\n",
    "    \"\"\"\n",
    "    Compute the unsupervised loss for GraphSAGE using contrastive negative sampling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : torch.Tensor\n",
    "        Node embeddings of shape `(num_nodes, embedding_dim)`.\n",
    "    edge_index : torch.Tensor\n",
    "        Graph connectivity matrix of shape `(2, num_edges)`, where each column represents an edge `(u, v)`.\n",
    "    num_neg_samples : int, optional\n",
    "        Number of negative samples per positive edge (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        The computed contrastive loss value.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The loss function follows the **GraphSAGE unsupervised learning approach**, leveraging contrastive learning.\n",
    "    - **Positive pairs**: Directly connected nodes in `edge_index`.\n",
    "    - **Negative pairs**: Randomly sampled nodes that are not neighbors.\n",
    "    - The objective is to maximize similarity for positive pairs and minimize it for negative pairs.\n",
    "    \"\"\"\n",
    "    pos_loss = torch.tensor(0.0, device=z.device)  # Loss for positive node pairs\n",
    "    neg_loss = torch.tensor(0.0, device=z.device)  # Loss for negative node pairs\n",
    "\n",
    "    num_nodes = z.shape[0]  # Number of nodes in the graph\n",
    "\n",
    "    for edge in edge_index.T:  # Iterate over each edge in the graph\n",
    "        u, v = edge  # Extract source node (u) and destination node (v)\n",
    "\n",
    "        # Positive pair loss (nodes that are neighbors)\n",
    "        pos_loss += torch.log(torch.sigmoid(torch.dot(z[u], z[v])))\n",
    "\n",
    "        # Negative sampling (random nodes that are NOT neighbors)\n",
    "        for _ in range(num_neg_samples):\n",
    "            v_neg = random.randint(0, num_nodes - 1)\n",
    "            while v_neg in edge_index[1]:  # Ensure v_neg is NOT a neighbor\n",
    "                v_neg = random.randint(0, num_nodes - 1)\n",
    "\n",
    "            neg_loss += torch.log(1 - torch.sigmoid(torch.dot(z[u], z[v_neg])))\n",
    "\n",
    "    loss = -(pos_loss + neg_loss) / edge_index.shape[1]  # Normalize by number of edges\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)  # Move batch to device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):  # mixed precission\n",
    "            z = model(batch.x, batch.edge_index)\n",
    "            loss = unsupervised_loss(z, batch.edge_index)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed.\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n",
      "Epoch 5 completed.\n",
      "Epoch 6 completed.\n",
      "Epoch 7 completed.\n",
      "Epoch 8 completed.\n",
      "Epoch 9 completed.\n",
      "Epoch 10 completed.\n",
      "Epoch 11 completed.\n",
      "Epoch 12 completed.\n",
      "Epoch 13 completed.\n",
      "Epoch 14 completed.\n",
      "Epoch 15 completed.\n",
      "Epoch 16 completed.\n",
      "Epoch 17 completed.\n",
      "Epoch 18 completed.\n",
      "Epoch 19 completed.\n",
      "Epoch 20 completed.\n",
      "Epoch 21 completed.\n",
      "Epoch 22 completed.\n",
      "Epoch 23 completed.\n",
      "Epoch 24 completed.\n",
      "Epoch 25 completed.\n",
      "Epoch 26 completed.\n",
      "Epoch 27 completed.\n",
      "Epoch 28 completed.\n",
      "Epoch 29 completed.\n",
      "Epoch 30 completed.\n",
      "Epoch 31 completed.\n",
      "Epoch 32 completed.\n",
      "Epoch 33 completed.\n",
      "Epoch 34 completed.\n",
      "Epoch 35 completed.\n",
      "Epoch 36 completed.\n",
      "Epoch 37 completed.\n",
      "Epoch 38 completed.\n",
      "Epoch 39 completed.\n",
      "Epoch 40 completed.\n",
      "Epoch 41 completed.\n",
      "Epoch 42 completed.\n",
      "Epoch 43 completed.\n",
      "Epoch 44 completed.\n",
      "Epoch 45 completed.\n",
      "Epoch 46 completed.\n",
      "Epoch 47 completed.\n",
      "Epoch 48 completed.\n",
      "Epoch 49 completed.\n",
      "Epoch 50 completed.\n",
      "Epoch 51 completed.\n",
      "Epoch 52 completed.\n",
      "Epoch 53 completed.\n",
      "Epoch 54 completed.\n",
      "Epoch 55 completed.\n",
      "Epoch 56 completed.\n",
      "Epoch 57 completed.\n",
      "Epoch 58 completed.\n",
      "Epoch 59 completed.\n",
      "Epoch 60 completed.\n",
      "Epoch 61 completed.\n",
      "Epoch 62 completed.\n",
      "Epoch 63 completed.\n",
      "Epoch 64 completed.\n",
      "Epoch 65 completed.\n",
      "Epoch 66 completed.\n",
      "Epoch 67 completed.\n",
      "Epoch 68 completed.\n",
      "Epoch 69 completed.\n",
      "Epoch 70 completed.\n",
      "Epoch 71 completed.\n",
      "Epoch 72 completed.\n",
      "Epoch 73 completed.\n",
      "Epoch 74 completed.\n",
      "Epoch 75 completed.\n",
      "Epoch 76 completed.\n",
      "Epoch 77 completed.\n",
      "Epoch 78 completed.\n",
      "Epoch 79 completed.\n",
      "Epoch 80 completed.\n",
      "Epoch 81 completed.\n",
      "Epoch 82 completed.\n",
      "Epoch 83 completed.\n",
      "Epoch 84 completed.\n",
      "Epoch 85 completed.\n",
      "Epoch 86 completed.\n",
      "Epoch 87 completed.\n",
      "Epoch 88 completed.\n",
      "Epoch 89 completed.\n",
      "Epoch 90 completed.\n",
      "Epoch 91 completed.\n",
      "Epoch 92 completed.\n",
      "Epoch 93 completed.\n",
      "Epoch 94 completed.\n",
      "Epoch 95 completed.\n",
      "Epoch 96 completed.\n",
      "Epoch 97 completed.\n",
      "Epoch 98 completed.\n",
      "Epoch 99 completed.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  # Number of training epochs (Each epoch takes apr. 25-30 seconds)\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    train()\n",
    "    print(f\"Epoch {epoch} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings as .npy file\n",
    "\n",
    "## 1) Get the device from any parameter in the model\n",
    "#device = next(model.parameters()).device\n",
    "#\n",
    "## 2) Now move your data.x and data.edge_index to that device:\n",
    "#data_x = data.x.to(device)\n",
    "#data_edge_index = data.edge_index.to(device)\n",
    "#\n",
    "## 3) Forward pass with torch.no_grad():\n",
    "#with torch.no_grad():\n",
    "#    embeddings = model(data_x, data_edge_index)\n",
    "#\n",
    "#embeddings_np = embeddings.cpu().numpy()\n",
    "#np.save(\"../data/graphsage_embeddings.npy\", embeddings_np)\n",
    "#print(\"Embeddings saved to graphsage_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE embeddings added to G under 'SAGE_embedding' attribute.\n"
     ]
    }
   ],
   "source": [
    "# Add Graph_SAGE embeddings to the baseline graph\n",
    "\n",
    "# 1) Move data to the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "data_x = data.x.to(device)\n",
    "data_edge_index = data.edge_index.to(device)\n",
    "\n",
    "# 2) Obtain final embeddings from the trained model\n",
    "with torch.no_grad():\n",
    "    final_emb = model(data_x, data_edge_index)  # shape [num_nodes, embedding_dim]\n",
    "    final_emb_np = final_emb.cpu().numpy()\n",
    "\n",
    "# 3) Add them back to the cleaned_G graph\n",
    "list_of_nodes = list(G.nodes())  # Must match the node ordering in data\n",
    "for i, node in enumerate(list_of_nodes):\n",
    "    # Store as a NumPy array (or you could store as a list if you prefer)\n",
    "    G.nodes[node][\"SAGE_embedding\"] = final_emb_np[i]\n",
    "\n",
    "print(\"SAGE embeddings added to G under 'SAGE_embedding' attribute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/MultiHop_graph_w_sage{num_epochs}_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(G, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
